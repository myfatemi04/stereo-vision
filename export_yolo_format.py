# Will store the dataset in COCO format.
# Using a more powerful labeler with a bit of manual feedback.

import logging
import math
import torch
import cv2
import os
import json

import tqdm

def convert_boxes(boxes: torch.Tensor):
    # (center x, center y, width, height) -> (x1, y1, x2, y2)
    cx, cy, w, h = boxes.unbind(-1)
    x1 = cx - 0.5 * w
    y1 = cy - 0.5 * h
    x2 = cx + 0.5 * w
    y2 = cy + 0.5 * h

    boxes = torch.stack((x1, y1, x2, y2), dim=-1)
    return boxes

def do_export(bag_id, camera_name, output_dir):
    CAR_CLASS_ID = 0

    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(message)s')

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    foundation_model_annotations_path = f'{bag_id}/{camera_name}_results.pt'
    foundation_model_annotations = torch.load(foundation_model_annotations_path)

    logging.info("Loaded results from %s", foundation_model_annotations_path)

    cap = cv2.VideoCapture(f"{bag_id}/{camera_name}.mp4")

    # Define a set of points that are on *our* car.
    # If the bounding box contains these points, we
    # filter this box out as "not an opponent".
    # Stores as paralle tensors.
    forbidden_points = [(2000, 900)]
    forbidden_x = torch.tensor([x for (x, _) in forbidden_points]) / 2064
    forbidden_y = torch.tensor([y for (_, y) in forbidden_points]) / 960

    torch.random.manual_seed(42)

    # Exports to a YOLOv5 folder structure.
    # $DATASET_ROOT/{train, test, val}/{images, labels}/{image_id}.{png, txt}

    all_image_ids = list(range(len(foundation_model_annotations)))
    NUM_FOLDS = 5
    fold_size = math.ceil(len(all_image_ids) / NUM_FOLDS)

    for image_id, (boxes, confidences, labels) in tqdm.tqdm(enumerate(foundation_model_annotations), desc='Saving images and annotations', total=len(foundation_model_annotations)):
        _, frame = cap.read()
        h, w, _ = frame.shape

        fold_id = image_id // fold_size
        fold_subdir = f'fold_{fold_id}'
        fold_path = os.path.join(output_dir, "cameras", camera_name, "folds", fold_subdir)
        if not os.path.exists(fold_path):
            os.makedirs(os.path.join(fold_path, "images"))
            os.makedirs(os.path.join(fold_path, "labels"))

        scale = torch.tensor([w, h, w, h])
        # Boxes use proportional coordinates
        boxes = convert_boxes(boxes)

        # Remove ``boxes" that cover the entire screen.
        areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
        boxes = boxes[areas < 0.9]

        # Remove boxes containing forbidden points.
        contains_forbidden_x = (boxes[:, 0] < forbidden_x) & (forbidden_x < boxes[:, 2])
        contains_forbidden_y = (boxes[:, 1] < forbidden_y) & (forbidden_y < boxes[:, 3])
        contains_forbidden = contains_forbidden_x & contains_forbidden_y
        boxes = boxes[~contains_forbidden]

        # Use consistent image_id (index within original video file)
        # Only write images to disk if they do not exist there already.
        image_path = os.path.join(fold_path, "images", "image_%d.png" % image_id)
        if not os.path.exists(image_path):
            cv2.imwrite(image_path, frame)

        # Write labels.
        labels_path = os.path.join(fold_path, "labels", "image_%d.txt" % image_id)
        labels_string = ''
        for box in boxes:
            # Do not scale to original size; use proportional coordinates.
            x1, y1, x2, y2 = box
            # Format: x_center, y_center, width, height
            x_center = (x1 + x2) / 2
            y_center = (y1 + y2) / 2
            width = x2 - x1
            height = y2 - y1
            labels_string += f"{CAR_CLASS_ID} {x_center} {y_center} {width} {height}\n"
        
        # Write labels to disk.
        with open(labels_path, 'w') as f:
            f.write(labels_string)

    # Store dataset YAML file.
    yaml = f"""
# NOTE: This is an autogenerated file. Created by `export_yolo_format.py`.
# You may need to change the `path` value below. It should be a path relative to the `data` directory in the yolov5 repo.
# bag_id: {bag_id}
# camera_name: {camera_name}

path: ../{output_dir}  # dataset root dir
# You can update these.
# `train` and `val` are (relative to 'path')
train:
 - cameras/{camera_name}/folds/fold_0/images/
 - cameras/{camera_name}/folds/fold_1/images/
 - cameras/{camera_name}/folds/fold_2/images/
 - cameras/{camera_name}/folds/fold_3/images/
# Validation images
val:
 - cameras/{camera_name}/folds/fold_4/images/
# Optional
test:

# Classes
names:
  {CAR_CLASS_ID}: car
""".strip() + "\n"
    
    with open(os.path.join(output_dir, f"dataset_{camera_name}.yaml"), 'w') as f:
        f.write(yaml)
    
def main():
    bag_id = 'M-MULTI-SLOW-KAIST'
    output_dir = "M-MULTI-SLOW-KAIST_yolo_format_for_cross_validation"
    for camera_name in ['front_left_center']:
        do_export(bag_id, camera_name, output_dir)

if __name__ == '__main__':
    main()
